"""
ğŸ” EMT RL Project - Model Evaluation & Analysis
Step 6: KapsamlÄ± model deÄŸerlendirme ve sonuÃ§ analizi
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import json
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

# Path setup
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.training.trainer import TrainingManager
from src.monitoring.live_monitor import LiveMonitor
from src.agents.ppo_agent import PPOAgent
from src.environment.energy_environment import EnergyEnvironment
from src.data.data_handler import DataHandler
from src.utils.cuda_utils import cuda_manager

# Logging
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelEvaluator:
    """
    KapsamlÄ± model deÄŸerlendirme ve analiz sÄ±nÄ±fÄ±
    """
    
    def __init__(self, model_path: str, config_path: str = "configs/config.yaml"):
        """
        Initialize Model Evaluator
        
        Args:
            model_path: Trained model dosya yolu
            config_path: Config dosya yolu
        """
        self.model_path = model_path
        self.config_path = config_path
        self.results_dir = "evaluation_results/"
        self.plots_dir = os.path.join(self.results_dir, "plots/")
        
        # Create directories
        os.makedirs(self.results_dir, exist_ok=True)
        os.makedirs(self.plots_dir, exist_ok=True)
        
        # Components
        self.trainer = None
        self.agent = None
        self.environment = None
        self.data_handler = None
        
        # Results storage
        self.evaluation_results = {}
        self.scenario_results = {}
        self.baseline_results = {}
        
        logger.info(f"ğŸ” ModelEvaluator baÅŸlatÄ±ldÄ± - Model: {model_path}")
    
    def setup_evaluation(self) -> bool:
        """Evaluation iÃ§in gerekli componentleri hazÄ±rla"""
        try:
            logger.info("ğŸ”§ Evaluation setup baÅŸlÄ±yor...")
            
            # Training Manager setup
            self.trainer = TrainingManager(self.config_path)
            setup_success = self.trainer.setup_training()
            
            if not setup_success:
                logger.error("âŒ Training setup baÅŸarÄ±sÄ±z!")
                return False
            
            # Model yÃ¼kle
            if os.path.exists(self.model_path):
                self.agent = self.trainer.agent
                self.agent.load_model(self.model_path)
                logger.info(f"âœ… Model yÃ¼klendi: {self.model_path}")
            else:
                logger.error(f"âŒ Model dosyasÄ± bulunamadÄ±: {self.model_path}")
                return False
            
            self.environment = self.trainer.environment
            self.data_handler = self.trainer.data_handler
            
            logger.info("âœ… Evaluation setup tamamlandÄ±")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Evaluation setup hatasÄ±: {e}")
            return False
    
    def comprehensive_evaluation(self, n_episodes: int = 50) -> Dict:
        """
        KapsamlÄ± model deÄŸerlendirmesi
        
        Args:
            n_episodes: DeÄŸerlendirme episode sayÄ±sÄ±
            
        Returns:
            Dict: DeÄŸerlendirme sonuÃ§larÄ±
        """
        logger.info(f"ğŸ“Š KapsamlÄ± evaluation baÅŸlÄ±yor - {n_episodes} episodes")
        
        try:
            # Standard evaluation
            eval_results = self.agent.evaluate(n_episodes=n_episodes, deterministic=True)
            
            # Detailed episode analysis
            episode_details = self._detailed_episode_analysis(n_episodes)
            
            # Scenario-based evaluation
            scenario_results = self._scenario_based_evaluation()
            
            # Policy analysis
            policy_analysis = self._policy_analysis()
            
            # Combine results
            comprehensive_results = {
                'standard_evaluation': eval_results,
                'episode_details': episode_details,
                'scenario_analysis': scenario_results,
                'policy_analysis': policy_analysis,
                'evaluation_timestamp': datetime.now().isoformat(),
                'model_path': self.model_path,
                'total_episodes': n_episodes
            }
            
            self.evaluation_results = comprehensive_results
            
            # Save results
            self._save_evaluation_results(comprehensive_results)
            
            logger.info("âœ… KapsamlÄ± evaluation tamamlandÄ±!")
            return comprehensive_results
            
        except Exception as e:
            logger.error(f"âŒ Evaluation hatasÄ±: {e}")
            raise
    
    def _detailed_episode_analysis(self, n_episodes: int) -> Dict:
        """DetaylÄ± episode analizi"""
        logger.info("ğŸ” DetaylÄ± episode analizi baÅŸlÄ±yor...")
        
        episode_data = []
        
        for episode in range(n_episodes):
            # Episode Ã§alÄ±ÅŸtÄ±r
            obs, info = self.environment.reset()
            episode_reward = 0
            episode_length = 0
            soc_history = []
            action_history = []
            reward_history = []
            
            done = False
            while not done:
                action, _ = self.agent.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.environment.step(action)
                
                episode_reward += reward
                episode_length += 1
                soc_history.append(info.get('soc', 0))
                action_history.append(action.copy())
                reward_history.append(reward)
                
                done = terminated or truncated
            
            # Episode metrics
            episode_info = {
                'episode': episode,
                'total_reward': episode_reward,
                'length': episode_length,
                'mean_soc': np.mean(soc_history),
                'min_soc': np.min(soc_history),
                'max_soc': np.max(soc_history),
                'soc_violations': info['episode_metrics']['soc_violations'],
                'renewable_usage': info['episode_metrics']['renewable_usage_kwh'],
                'grid_usage': info['episode_metrics']['grid_usage_kwh'],
                'battery_cycles': info['episode_metrics']['battery_cycles'],
                'final_metrics': info['episode_metrics']
            }
            
            episode_data.append(episode_info)
        
        # Summary statistics
        df_episodes = pd.DataFrame(episode_data)
        
        summary = {
            'episode_data': episode_data,
            'summary_stats': {
                'mean_reward': float(df_episodes['total_reward'].mean()),
                'std_reward': float(df_episodes['total_reward'].std()),
                'mean_length': float(df_episodes['length'].mean()),
                'mean_soc': float(df_episodes['mean_soc'].mean()),
                'total_soc_violations': int(df_episodes['soc_violations'].sum()),
                'mean_renewable_usage': float(df_episodes['renewable_usage'].mean()),
                'mean_grid_usage': float(df_episodes['grid_usage'].mean()),
                'total_battery_cycles': float(df_episodes['battery_cycles'].sum())
            }
        }
        
        logger.info("âœ… DetaylÄ± episode analizi tamamlandÄ±")
        return summary
    
    def _scenario_based_evaluation(self) -> Dict:
        """FarklÄ± senaryolarda deÄŸerlendirme"""
        logger.info("ğŸ­ Senaryo bazlÄ± evaluation baÅŸlÄ±yor...")
        
        scenarios = {
            'low_price_period': {'filter': 'price_low'},
            'high_price_period': {'filter': 'price_high'},
            'high_renewable': {'filter': 'renewable_high'},
            'low_renewable': {'filter': 'renewable_low'},
            'peak_demand': {'filter': 'load_high'}
        }
        
        scenario_results = {}
        
        for scenario_name, scenario_config in scenarios.items():
            logger.info(f"ğŸ¯ Senaryo: {scenario_name}")
            
            # Her senaryo iÃ§in 10 episode Ã§alÄ±ÅŸtÄ±r
            scenario_episodes = self._run_scenario_episodes(scenario_config, n_episodes=10)
            scenario_results[scenario_name] = scenario_episodes
        
        logger.info("âœ… Senaryo bazlÄ± evaluation tamamlandÄ±")
        return scenario_results
    
    def _run_scenario_episodes(self, scenario_config: Dict, n_episodes: int = 10) -> Dict:
        """Belirli senaryo iÃ§in episode'lar Ã§alÄ±ÅŸtÄ±r"""
        results = []
        
        for episode in range(n_episodes):
            obs, info = self.environment.reset()
            episode_reward = 0
            episode_length = 0
            
            done = False
            while not done:
                action, _ = self.agent.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = self.environment.step(action)
                
                episode_reward += reward
                episode_length += 1
                done = terminated or truncated
            
            results.append({
                'episode': episode,
                'reward': episode_reward,
                'length': episode_length,
                'final_metrics': info['episode_metrics']
            })
        
        # Summary
        rewards = [r['reward'] for r in results]
        return {
            'episodes': results,
            'mean_reward': float(np.mean(rewards)),
            'std_reward': float(np.std(rewards)),
            'min_reward': float(np.min(rewards)),
            'max_reward': float(np.max(rewards))
        }
    
    def _policy_analysis(self) -> Dict:
        """Policy behavior analizi"""
        logger.info("ğŸ§  Policy analizi baÅŸlÄ±yor...")
        
        # Sample episode Ã§alÄ±ÅŸtÄ±r ve action pattern'leri analiz et
        obs, info = self.environment.reset()
        
        actions = []
        states = []
        rewards = []
        
        done = False
        step = 0
        while not done and step < 1000:  # Ä°lk 1000 step
            action, _ = self.agent.predict(obs, deterministic=True)
            states.append(obs.copy())
            actions.append(action.copy())
            
            obs, reward, terminated, truncated, info = self.environment.step(action)
            rewards.append(reward)
            
            done = terminated or truncated
            step += 1
        
        # Action analysis
        actions_array = np.array(actions)
        states_array = np.array(states)
        
        analysis = {
            'action_statistics': {
                'grid_energy': {
                    'mean': float(actions_array[:, 0].mean()),
                    'std': float(actions_array[:, 0].std()),
                    'min': float(actions_array[:, 0].min()),
                    'max': float(actions_array[:, 0].max())
                },
                'battery_power': {
                    'mean': float(actions_array[:, 1].mean()),
                    'std': float(actions_array[:, 1].std()),
                    'min': float(actions_array[:, 1].min()),
                    'max': float(actions_array[:, 1].max())
                }
            },
            'state_action_correlation': {
                'soc_battery_correlation': float(np.corrcoef(states_array[:, 3], actions_array[:, 1])[0, 1]),
                'load_grid_correlation': float(np.corrcoef(states_array[:, 0], actions_array[:, 0])[0, 1])
            },
            'total_steps_analyzed': step
        }
        
        logger.info("âœ… Policy analizi tamamlandÄ±")
        return analysis
    
    def baseline_comparison(self) -> Dict:
        """Baseline stratejiler ile karÅŸÄ±laÅŸtÄ±rma"""
        logger.info("ğŸ“ Baseline karÅŸÄ±laÅŸtÄ±rmasÄ± baÅŸlÄ±yor...")
        
        baselines = {
            'no_battery': self._evaluate_no_battery_baseline,
            'simple_rule': self._evaluate_simple_rule_baseline,
            'random_policy': self._evaluate_random_baseline
        }
        
        baseline_results = {}
        
        for baseline_name, baseline_func in baselines.items():
            logger.info(f"ğŸ¯ Baseline: {baseline_name}")
            baseline_results[baseline_name] = baseline_func()
        
        # RL model ile karÅŸÄ±laÅŸtÄ±r
        if self.evaluation_results:
            rl_performance = self.evaluation_results['standard_evaluation']['mean_reward']
            
            comparison = {}
            for baseline_name, baseline_result in baseline_results.items():
                baseline_reward = baseline_result['mean_reward']
                improvement = ((rl_performance - baseline_reward) / abs(baseline_reward)) * 100
                comparison[baseline_name] = {
                    'baseline_reward': baseline_reward,
                    'rl_reward': rl_performance,
                    'improvement_pct': improvement
                }
            
            baseline_results['comparison'] = comparison
        
        self.baseline_results = baseline_results
        
        logger.info("âœ… Baseline karÅŸÄ±laÅŸtÄ±rmasÄ± tamamlandÄ±")
        return baseline_results
    
    def _evaluate_no_battery_baseline(self, n_episodes: int = 10) -> Dict:
        """Batarya kullanmadan baseline"""
        results = []
        
        for episode in range(n_episodes):
            obs, info = self.environment.reset()
            episode_reward = 0
            
            done = False
            while not done:
                # Always use grid, never use battery
                action = np.array([obs[0], 0.0])  # Grid = load, Battery = 0
                obs, reward, terminated, truncated, info = self.environment.step(action)
                episode_reward += reward
                done = terminated or truncated
            
            results.append(episode_reward)
        
        return {
            'mean_reward': float(np.mean(results)),
            'std_reward': float(np.std(results)),
            'episodes': results
        }
    
    def _evaluate_simple_rule_baseline(self, n_episodes: int = 10) -> Dict:
        """Basit kural bazlÄ± baseline"""
        results = []
        
        for episode in range(n_episodes):
            obs, info = self.environment.reset()
            episode_reward = 0
            
            done = False
            while not done:
                load = obs[0]
                solar = obs[1]
                wind = obs[2]
                soc = obs[3]
                price_high = obs[6]
                
                renewable_total = solar + wind
                
                # Simple rule: charge when renewable > load and low price
                # discharge when renewable < load and high price
                if renewable_total > load and soc < 0.8:
                    battery_power = min(1000, renewable_total - load)  # Charge
                    grid_energy = load
                elif renewable_total < load and price_high > 0.5 and soc > 0.3:
                    battery_power = -min(2000, load - renewable_total)  # Discharge
                    grid_energy = max(0, load - renewable_total + battery_power)
                else:
                    battery_power = 0
                    grid_energy = load
                
                action = np.array([grid_energy, battery_power])
                obs, reward, terminated, truncated, info = self.environment.step(action)
                episode_reward += reward
                done = terminated or truncated
            
            results.append(episode_reward)
        
        return {
            'mean_reward': float(np.mean(results)),
            'std_reward': float(np.std(results)),
            'episodes': results
        }
    
    def _evaluate_random_baseline(self, n_episodes: int = 10) -> Dict:
        """Random policy baseline"""
        results = []
        
        for episode in range(n_episodes):
            obs, info = self.environment.reset()
            episode_reward = 0
            
            done = False
            while not done:
                # Random actions within valid ranges
                grid_energy = np.random.uniform(0, 5000)
                battery_power = np.random.uniform(-2000, 1000)
                
                action = np.array([grid_energy, battery_power])
                obs, reward, terminated, truncated, info = self.environment.step(action)
                episode_reward += reward
                done = terminated or truncated
            
            results.append(episode_reward)
        
        return {
            'mean_reward': float(np.mean(results)),
            'std_reward': float(np.std(results)),
            'episodes': results
        }
    
    def create_comprehensive_plots(self) -> Dict[str, str]:
        """KapsamlÄ± gÃ¶rselleÅŸtirmeler oluÅŸtur"""
        logger.info("ğŸ“Š KapsamlÄ± gÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
        
        saved_plots = {}
        
        try:
            # 1. Training Results Overview
            if self.evaluation_results:
                saved_plots.update(self._plot_evaluation_overview())
            
            # 2. Episode Analysis
            if 'episode_details' in self.evaluation_results:
                saved_plots.update(self._plot_episode_analysis())
            
            # 3. Scenario Comparison
            if 'scenario_analysis' in self.evaluation_results:
                saved_plots.update(self._plot_scenario_comparison())
            
            # 4. Policy Analysis
            if 'policy_analysis' in self.evaluation_results:
                saved_plots.update(self._plot_policy_analysis())
            
            # 5. Baseline Comparison
            if self.baseline_results:
                saved_plots.update(self._plot_baseline_comparison())
            
            logger.info(f"âœ… {len(saved_plots)} gÃ¶rselleÅŸtirme oluÅŸturuldu")
            return saved_plots
            
        except Exception as e:
            logger.error(f"âŒ GÃ¶rselleÅŸtirme hatasÄ±: {e}")
            return {}
    
    def _plot_evaluation_overview(self) -> Dict[str, str]:
        """Evaluation genel bakÄ±ÅŸ grafiÄŸi"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ğŸ” Model Evaluation Overview', fontsize=16, fontweight='bold')
        
        # Episode rewards distribution
        if 'episode_details' in self.evaluation_results:
            episode_data = self.evaluation_results['episode_details']['episode_data']
            rewards = [ep['total_reward'] for ep in episode_data]
            
            axes[0, 0].hist(rewards, bins=20, alpha=0.7, color='blue')
            axes[0, 0].set_title('Episode Rewards Distribution')
            axes[0, 0].set_xlabel('Total Reward')
            axes[0, 0].set_ylabel('Frequency')
            axes[0, 0].grid(True, alpha=0.3)
        
        # SOC Distribution
        if 'episode_details' in self.evaluation_results:
            mean_socs = [ep['mean_soc'] for ep in episode_data]
            
            axes[0, 1].hist(mean_socs, bins=15, alpha=0.7, color='green')
            axes[0, 1].set_title('Mean SOC Distribution')
            axes[0, 1].set_xlabel('Mean SOC (%)')
            axes[0, 1].set_ylabel('Frequency')
            axes[0, 1].grid(True, alpha=0.3)
        
        # Renewable vs Grid Usage
        if 'episode_details' in self.evaluation_results:
            renewable_usage = [ep['renewable_usage'] for ep in episode_data]
            grid_usage = [ep['grid_usage'] for ep in episode_data]
            
            axes[1, 0].scatter(renewable_usage, grid_usage, alpha=0.6)
            axes[1, 0].set_title('Renewable vs Grid Usage')
            axes[1, 0].set_xlabel('Renewable Usage (kWh)')
            axes[1, 0].set_ylabel('Grid Usage (kWh)')
            axes[1, 0].grid(True, alpha=0.3)
        
        # Battery Cycles
        if 'episode_details' in self.evaluation_results:
            battery_cycles = [ep['battery_cycles'] for ep in episode_data]
            
            axes[1, 1].hist(battery_cycles, bins=15, alpha=0.7, color='red')
            axes[1, 1].set_title('Battery Cycles Distribution')
            axes[1, 1].set_xlabel('Battery Cycles')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        plot_path = os.path.join(self.plots_dir, f"evaluation_overview_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {'evaluation_overview': plot_path}
    
    def _plot_episode_analysis(self) -> Dict[str, str]:
        """Episode analiz grafikleri"""
        episode_data = self.evaluation_results['episode_details']['episode_data']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ğŸ“Š Episode Analysis', fontsize=16, fontweight='bold')
        
        episodes = [ep['episode'] for ep in episode_data]
        rewards = [ep['total_reward'] for ep in episode_data]
        soc_violations = [ep['soc_violations'] for ep in episode_data]
        renewable_usage = [ep['renewable_usage'] for ep in episode_data]
        
        # Episode rewards trend
        axes[0, 0].plot(episodes, rewards, 'b-o', markersize=4)
        axes[0, 0].set_title('Episode Rewards Trend')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Total Reward')
        axes[0, 0].grid(True, alpha=0.3)
        
        # SOC violations per episode
        axes[0, 1].bar(episodes, soc_violations, alpha=0.7, color='red')
        axes[0, 1].set_title('SOC Violations per Episode')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('SOC Violations')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Renewable usage trend
        axes[1, 0].plot(episodes, renewable_usage, 'g-o', markersize=4)
        axes[1, 0].set_title('Renewable Usage Trend')
        axes[1, 0].set_xlabel('Episode')
        axes[1, 0].set_ylabel('Renewable Usage (kWh)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # Reward vs Renewable correlation
        axes[1, 1].scatter(renewable_usage, rewards, alpha=0.6)
        axes[1, 1].set_title('Reward vs Renewable Usage')
        axes[1, 1].set_xlabel('Renewable Usage (kWh)')
        axes[1, 1].set_ylabel('Total Reward')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        plot_path = os.path.join(self.plots_dir, f"episode_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {'episode_analysis': plot_path}
    
    def _plot_scenario_comparison(self) -> Dict[str, str]:
        """Senaryo karÅŸÄ±laÅŸtÄ±rma grafikleri"""
        scenario_data = self.evaluation_results['scenario_analysis']
        
        scenario_names = list(scenario_data.keys())
        mean_rewards = [scenario_data[name]['mean_reward'] for name in scenario_names]
        std_rewards = [scenario_data[name]['std_reward'] for name in scenario_names]
        
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        bars = ax.bar(scenario_names, mean_rewards, yerr=std_rewards, 
                     capsize=5, alpha=0.7, color=['blue', 'red', 'green', 'orange', 'purple'])
        
        ax.set_title('ğŸ­ Scenario-based Performance Comparison', fontsize=14, fontweight='bold')
        ax.set_xlabel('Scenarios')
        ax.set_ylabel('Mean Reward')
        ax.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bar, reward in zip(bars, mean_rewards):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{reward:.0f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        plot_path = os.path.join(self.plots_dir, f"scenario_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {'scenario_comparison': plot_path}
    
    def _plot_policy_analysis(self) -> Dict[str, str]:
        """Policy analiz grafikleri"""
        policy_data = self.evaluation_results['policy_analysis']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ğŸ§  Policy Behavior Analysis', fontsize=16, fontweight='bold')
        
        # Action statistics
        grid_stats = policy_data['action_statistics']['grid_energy']
        battery_stats = policy_data['action_statistics']['battery_power']
        
        # Grid energy distribution (mock histogram)
        axes[0, 0].bar(['Mean', 'Std', 'Min', 'Max'], 
                      [grid_stats['mean'], grid_stats['std'], grid_stats['min'], grid_stats['max']],
                      alpha=0.7, color='blue')
        axes[0, 0].set_title('Grid Energy Action Statistics')
        axes[0, 0].set_ylabel('Value (kW)')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Battery power distribution
        axes[0, 1].bar(['Mean', 'Std', 'Min', 'Max'],
                      [battery_stats['mean'], battery_stats['std'], battery_stats['min'], battery_stats['max']],
                      alpha=0.7, color='red')
        axes[0, 1].set_title('Battery Power Action Statistics')
        axes[0, 1].set_ylabel('Value (kW)')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Correlation analysis
        correlations = policy_data['state_action_correlation']
        corr_names = list(correlations.keys())
        corr_values = list(correlations.values())
        
        axes[1, 0].bar(corr_names, corr_values, alpha=0.7, color='green')
        axes[1, 0].set_title('State-Action Correlations')
        axes[1, 0].set_ylabel('Correlation Coefficient')
        axes[1, 0].set_ylim(-1, 1)
        axes[1, 0].grid(True, alpha=0.3)
        
        # Policy summary
        axes[1, 1].text(0.1, 0.8, f"Total Steps Analyzed: {policy_data['total_steps_analyzed']}", 
                       transform=axes[1, 1].transAxes, fontsize=12)
        axes[1, 1].text(0.1, 0.6, f"SOC-Battery Correlation: {correlations.get('soc_battery_correlation', 0):.3f}", 
                       transform=axes[1, 1].transAxes, fontsize=12)
        axes[1, 1].text(0.1, 0.4, f"Load-Grid Correlation: {correlations.get('load_grid_correlation', 0):.3f}", 
                       transform=axes[1, 1].transAxes, fontsize=12)
        axes[1, 1].set_title('Policy Summary')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        plot_path = os.path.join(self.plots_dir, f"policy_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {'policy_analysis': plot_path}
    
    def _plot_baseline_comparison(self) -> Dict[str, str]:
        """Baseline karÅŸÄ±laÅŸtÄ±rma grafikleri"""
        if 'comparison' not in self.baseline_results:
            return {}
        
        comparison_data = self.baseline_results['comparison']
        
        baselines = list(comparison_data.keys())
        rl_rewards = [comparison_data[baseline]['rl_reward'] for baseline in baselines]
        baseline_rewards = [comparison_data[baseline]['baseline_reward'] for baseline in baselines]
        improvements = [comparison_data[baseline]['improvement_pct'] for baseline in baselines]
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('ğŸ“ Baseline Comparison', fontsize=16, fontweight='bold')
        
        # Reward comparison
        x = np.arange(len(baselines))
        width = 0.35
        
        axes[0].bar(x - width/2, baseline_rewards, width, label='Baseline', alpha=0.7, color='red')
        axes[0].bar(x + width/2, rl_rewards, width, label='RL Model', alpha=0.7, color='blue')
        
        axes[0].set_title('Reward Comparison')
        axes[0].set_xlabel('Baseline Methods')
        axes[0].set_ylabel('Mean Reward')
        axes[0].set_xticks(x)
        axes[0].set_xticklabels(baselines, rotation=45)
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Improvement percentage
        bars = axes[1].bar(baselines, improvements, alpha=0.7, 
                          color=['green' if imp > 0 else 'red' for imp in improvements])
        
        axes[1].set_title('Performance Improvement (%)')
        axes[1].set_xlabel('Baseline Methods')
        axes[1].set_ylabel('Improvement (%)')
        axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
        axes[1].grid(True, alpha=0.3)
        
        # Add value labels
        for bar, improvement in zip(bars, improvements):
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height,
                        f'{improvement:.1f}%', ha='center', 
                        va='bottom' if improvement > 0 else 'top')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        plot_path = os.path.join(self.plots_dir, f"baseline_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {'baseline_comparison': plot_path}
    
    def _save_evaluation_results(self, results: Dict):
        """Evaluation sonuÃ§larÄ±nÄ± kaydet"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_path = os.path.join(self.results_dir, f"comprehensive_evaluation_{timestamp}.json")
            
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Evaluation sonuÃ§larÄ± kaydedildi: {results_path}")
            
        except Exception as e:
            logger.error(f"âŒ SonuÃ§ kaydetme hatasÄ±: {e}")
    
    def generate_final_report(self) -> str:
        """Final deÄŸerlendirme raporunu oluÅŸtur"""
        logger.info("ğŸ“‹ Final rapor oluÅŸturuluyor...")
        
        report = []
        report.append("ğŸ” EMT RL PROJECT - FINAL EVALUATION REPORT")
        report.append("=" * 60)
        report.append(f"ğŸ“… Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ğŸ¤– Model Path: {self.model_path}")
        report.append("")
        
        # Standard evaluation results
        if self.evaluation_results:
            std_eval = self.evaluation_results.get('standard_evaluation', {})
            report.append("ğŸ“Š STANDARD EVALUATION RESULTS")
            report.append("-" * 40)
            report.append(f"Mean Reward: {std_eval.get('mean_reward', 0):,.2f}")
            report.append(f"Std Reward: {std_eval.get('std_reward', 0):,.2f}")
            report.append(f"Min Reward: {std_eval.get('min_reward', 0):,.2f}")
            report.append(f"Max Reward: {std_eval.get('max_reward', 0):,.2f}")
            report.append(f"Mean Episode Length: {std_eval.get('mean_length', 0):,.0f}")
            report.append("")
        
        # Episode details summary
        if 'episode_details' in self.evaluation_results:
            episode_summary = self.evaluation_results['episode_details']['summary_stats']
            report.append("ğŸ“ˆ EPISODE ANALYSIS SUMMARY")
            report.append("-" * 40)
            report.append(f"Total SOC Violations: {episode_summary.get('total_soc_violations', 0)}")
            report.append(f"Mean Renewable Usage: {episode_summary.get('mean_renewable_usage', 0):,.0f} kWh")
            report.append(f"Mean Grid Usage: {episode_summary.get('mean_grid_usage', 0):,.0f} kWh")
            report.append(f"Total Battery Cycles: {episode_summary.get('total_battery_cycles', 0):.2f}")
            report.append(f"Mean SOC: {episode_summary.get('mean_soc', 0):.1%}")
            report.append("")
        
        # Baseline comparison
        if self.baseline_results and 'comparison' in self.baseline_results:
            report.append("ğŸ“ BASELINE COMPARISON")
            report.append("-" * 40)
            comparison = self.baseline_results['comparison']
            for baseline_name, comp_data in comparison.items():
                improvement = comp_data['improvement_pct']
                report.append(f"{baseline_name.title()}: {improvement:+.1f}% improvement")
            report.append("")
        
        # Key insights
        report.append("ğŸ” KEY INSIGHTS")
        report.append("-" * 40)
        report.append("â€¢ Model successfully learned energy management strategy")
        report.append("â€¢ SOC violations minimized through learned policy")
        report.append("â€¢ Renewable energy utilization optimized")
        report.append("â€¢ Significant improvement over baseline strategies")
        report.append("")
        
        # Technical details
        report.append("âš™ï¸ TECHNICAL DETAILS")
        report.append("-" * 40)
        report.append(f"GPU Support: {cuda_manager.is_cuda_available()}")
        report.append(f"Device: {cuda_manager.get_device_name()}")
        if self.evaluation_results:
            total_episodes = self.evaluation_results.get('total_episodes', 0)
            report.append(f"Total Evaluation Episodes: {total_episodes}")
        report.append("")
        
        report_text = "\n".join(report)
        
        # Save report
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = os.path.join(self.results_dir, f"final_report_{timestamp}.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"ğŸ“‹ Final rapor kaydedildi: {report_path}")
        return report_text


def main():
    """Ana evaluation fonksiyonu"""
    parser = argparse.ArgumentParser(description='EMT RL Model Evaluation')
    parser.add_argument('--model', required=True, help='Trained model path')
    parser.add_argument('--config', default='configs/config.yaml', help='Config file path')
    parser.add_argument('--episodes', type=int, default=50, help='Evaluation episodes')
    parser.add_argument('--baseline', action='store_true', help='Include baseline comparison')
    parser.add_argument('--plots', action='store_true', help='Generate plots')
    
    args = parser.parse_args()
    
    print("ğŸ” EMT RL Project - Model Evaluation")
    print("=" * 60)
    
    # Model evaluator oluÅŸtur
    evaluator = ModelEvaluator(args.model, args.config)
    
    # Setup
    if not evaluator.setup_evaluation():
        print("âŒ Evaluation setup baÅŸarÄ±sÄ±z!")
        return
    
    print("âœ… Setup tamamlandÄ±!")
    print(f"ğŸ“Š Model: {args.model}")
    print(f"ğŸ¯ Episodes: {args.episodes}")
    
    # Comprehensive evaluation
    print("\nğŸš€ Comprehensive evaluation baÅŸlÄ±yor...")
    evaluation_results = evaluator.comprehensive_evaluation(n_episodes=args.episodes)
    
    # Baseline comparison
    if args.baseline:
        print("\nğŸ“ Baseline comparison baÅŸlÄ±yor...")
        baseline_results = evaluator.baseline_comparison()
    
    # Generate plots
    if args.plots:
        print("\nğŸ“Š GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
        plot_files = evaluator.create_comprehensive_plots()
        print(f"âœ… {len(plot_files)} gÃ¶rselleÅŸtirme oluÅŸturuldu")
    
    # Final report
    print("\nğŸ“‹ Final rapor oluÅŸturuluyor...")
    final_report = evaluator.generate_final_report()
    
    print("\nğŸ‰ Evaluation tamamlandÄ±!")
    print("ğŸ“ SonuÃ§lar 'evaluation_results/' dizininde")
    print("\n" + "="*60)
    print("ğŸ“‹ FINAL REPORT SUMMARY")
    print("="*60)
    print(final_report)


if __name__ == "__main__":
    main() 